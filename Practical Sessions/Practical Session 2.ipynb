{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 Kernel Methods for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by Yunlong Jiao / Romain Menegaux, 19 May 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.16 (default, Oct 10 2019, 22:02:15) \n",
      "[GCC 8.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model as lm\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement (naive) solvers to Ridge Regression, Weighted Ridge Regression and Logistic Ridge Regression (using Iteratively Reweighted Least Squares). See notes for the mathematical derivation.\n",
    "2. Simulate some toy data to check if our solvers give correct solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "p = 10\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "beta_star = np.random.normal(0, 1, p)\n",
    "y = X.dot(beta_star) + 0.2 * np.random.normal(0, 1, n)\n",
    "\n",
    "def compare(beta1, beta2):\n",
    "    print('''\n",
    "Our solver:\n",
    "{}\n",
    "Scikit-learn:\n",
    "{}\n",
    "\n",
    "Difference between the two:\n",
    "{}\n",
    "        '''.format(beta1, beta2, np.sum((beta1-beta2)**2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression (RR)**\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\|y - X \\beta\\|^2 + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "The gradient (Jacobian) of the objective is :\n",
    "\n",
    "\\begin{equation}\\nabla f(\\beta)=\\frac{2}{n} X^{\\top}(X \\beta-y)+2 \\lambda \\beta\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\\nabla f(\\beta)=0 \\Longleftrightarrow\\left(X^{\\top} X+n \\lambda I\\right) \\beta=X^{\\top} y\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\beta}_{\\lambda}^{\\text {ridge }}=\\arg \\min _{\\beta \\in \\mathbb{R}^{p}}\\{R(\\beta)+\\lambda \\Omega(\\beta)\\}=\n",
    "    \\left(X^{\\top} X+\\lambda n I\\right)^{-1} X^{\\top} Y\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression (RR)\n",
    "def solveRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "    \n",
    "    # Hint:\n",
    "    # beta = np.linalg.solve(A, b)\n",
    "    # Finds solution to the linear system Ax = b\n",
    "    \n",
    "    beta = np.linalg.solve(X.T.dot(X) + lam * n*np.eye(p), X.T.dot(y))\n",
    "#     beta = np.linalg.inv(X.T.dot(X) + lam * n*np.eye(p)).dot(X.T.dot(y))\n",
    "\n",
    "\n",
    "    \n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our solver:\n",
      "[ 1.27929172  0.78935356  0.05064497 -0.55474398  0.65276533  0.32637554\n",
      "  0.765293    0.63326617  0.97285396 -0.5294559 ]\n",
      "Scikit-learn:\n",
      "[ 1.27929172  0.78935356  0.05064497 -0.55474398  0.65276533  0.32637554\n",
      "  0.765293    0.63326617  0.97285396 -0.5294559 ]\n",
      "\n",
      "Difference between the two:\n",
      "1.79496670817e-31\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "lam = 0.1\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveRR(y, X, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = lam * X.shape[0]\n",
    "model = lm.Ridge(alpha=alpha, fit_intercept=False, normalize=False)\n",
    "beta2 = model.fit(X, y).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Ridge Regression (WRR)**\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, and weights $w \\in \\mathbb{R}^n_+$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n w_i (y_i - \\beta^\\top x_i)^2 + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "Think of $w_i$ as importance or confidence you have in point $i$\n",
    "\n",
    "**Goal:** Express the objective as a regular Ridge Regression (RR)\n",
    "\n",
    "since the weights $w_{i}$ are non-negative, we can pull $w_{i}$ inside the parenthesis:\n",
    "\n",
    "$$\\sum_{i=1}^{n} w_{i}\\left(y_{i}-\\beta^{\\top} x_{i}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\sqrt{w_{i}} y_{i}-\\beta^{\\top} \\sqrt{w_{i}} x_{i}\\right)^{2}\n",
    "$$\n",
    "\n",
    "In matrix form:\n",
    "Introducing the diagonal matrix $W=\\operatorname{diag}\\left(w_{1}, \\ldots, w_{n}\\right),$ we can rewrite the objective\n",
    "$$\n",
    "\\sum_{i=1}^{n} w_{i}\\left(y_{i}-\\beta^{\\top} x_{i}\\right)^{2}=(Y-X \\beta)^{\\top} W(Y-X \\beta)\n",
    "$$\n",
    "\n",
    "We now write $W=W^{\\frac{1}{2}} W^{\\frac{1}{2}}=\\left(W^{\\frac{1}{2}}\\right)^{\\top} W^{\\frac{1}{2}},$ where $W^{\\frac{1}{2}}=\\operatorname{diag}(\\sqrt{w_{1}}, \\ldots, \\sqrt{w_{n}})$\n",
    "The objective becomes:\n",
    "$$\n",
    "\\frac{1}{n}\\left(W^{\\frac{1}{2}} Y-W^{\\frac{1}{2}} X \\beta\\right)^{\\top}\\left(W^{\\frac{1}{2}} Y-W^{\\frac{1}{2}} X \\beta\\right)+\\lambda\\|\\beta\\|^{2}=\\frac{1}{n}\\left\\|Y^{\\prime}-X^{\\prime} \\beta\\right\\|^{2}+\\lambda\\|\\beta\\|^{2}\n",
    "$$\n",
    "with $Y^{\\prime}=W^{\\frac{1}{2}} Y$ and $X^{\\prime}=W^{\\frac{1}{2}} X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Ridge Regression (WRR)\n",
    "def solveWRR(y, X, w, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == len(w) == n)\n",
    "\n",
    "    # Hint:\n",
    "    # Find y1 and X1 such that:\n",
    "    # beta = solveRR(y1, X1, lam)\n",
    "    w = np.diag(w) \n",
    "    y1 = np.dot(np.sqrt(w),y) \n",
    "    X1 = np.dot(np.sqrt(w), X) \n",
    "    \n",
    "    beta = solveRR(y1, X1, lam)\n",
    "    \n",
    "#     y1 = np.sqrt(w) * y\n",
    "#     X1 = (np.sqrt(w) * X.T).T\n",
    "#     # Hint:\n",
    "#     # Find y1 and X1 such that:\n",
    "#     beta = solveRR(y1, X1, lam)\n",
    "    \n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our solver:\n",
      "[ 1.2276039   0.69327332  0.04813036 -0.46707812  0.62676816  0.31099478\n",
      "  0.65612567  0.57589301  0.89624383 -0.50035963]\n",
      "Scikit-learn:\n",
      "[ 1.2276039   0.69327332  0.04813036 -0.46707812  0.62676816  0.31099478\n",
      "  0.65612567  0.57589301  0.89624383 -0.50035963]\n",
      "\n",
      "Difference between the two:\n",
      "2.27693067675e-31\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "lam = 0.1\n",
    "w = np.random.rand(len(y))\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveWRR(y, X, w, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = lam * X.shape[0]\n",
    "model = lm.Ridge(alpha=alpha, fit_intercept=False, normalize=False)\n",
    "beta2 = model.fit(X, y, sample_weight=w).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Ridge Regression (LRR)**\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\{-1,+1\\}^n$, solve\n",
    "$$\n",
    "\\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n \\log (1+e^{-y_i \\beta^\\top x_i}) + \\lambda \\|\\beta\\|^2 \\,.\n",
    "$$\n",
    "\n",
    "Let $\\sigma(x)=\\frac{1}{1+e^{-x}}$ be the sigmoid function\n",
    "Compute $\\sigma^{\\prime}(x)$\n",
    "$$\n",
    "\\sigma^{\\prime}(x)=\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}}=\\frac{1}{1+e^{-x}}-\\frac{1}{\\left(1+e^{-x}\\right)^{2}}=\\sigma(x)(1-\\sigma(x))=\\sigma(x) \\sigma(-x)\n",
    "$$\n",
    "\n",
    "Note: Under the logistic model, $\\sigma\\left(y_{\\dot{z}} \\beta^{\\top} x_{i}\\right)=\\mathbb{P}\\left[y=y_{i} | x_{i}, \\beta\\right]$\n",
    "\n",
    "$\\bullet \\mathbb{P}\\left[y_{i}=1 | x_{i}, \\beta\\right]=\\sigma\\left(\\beta^{\\top} x_{i}\\right)$ (definition of the model)\n",
    "\n",
    "$\\bullet \\mathbb{P}\\left[y_{i}=0 | x_{i}, \\beta\\right]=1-\\sigma\\left(\\beta^{\\top} x_{i}\\right)=\\sigma\\left(-\\beta^{\\top} x_{i}\\right)$\n",
    "\n",
    "\n",
    "Rewriting $J:$\n",
    "$$\n",
    "J(\\beta)=-\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left(\\sigma\\left(y_{i} \\beta^{\\top} x_{i}\\right)\\right)+\\lambda\\|\\beta\\|^{2}\n",
    "$$\n",
    "Compute its gradient $\\nabla J,$ and its Hessian $\\nabla^{2} J$\n",
    "\n",
    "Computing the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\beta)=-\\frac{1}{n_{i}} \\sum_{i=1}^{n} y_{i} \\sigma\\left(-y_{i} \\beta^{\\top} x_{i}\\right) x_{i}+2 \\lambda \\beta\n",
    "$$\n",
    "\n",
    "**Hessina matrix** \n",
    "\n",
    "$$\\nabla^2 J(\\beta) = \\nabla (\\nabla J(\\beta))$$\n",
    "\n",
    "$$\\nabla^{2} J(\\beta)=-\\frac{1}{n} \\sum_{i=1}^{n} \\sigma\\left(y_{i} \\beta^{\\top} x_{i}\\right) \\sigma\\left(-y_{i} \\beta^{\\top} x_{i}\\right) x_{i} x_{i}^{\\top}+2 \\lambda I$$\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\text { Define } w_{i}=\\sigma\\left(y_{i} \\beta^{\\top} x_{i}\\right) \\sigma\\left(-y_{i} \\beta^{\\top} x_{i}\\right) \\text { and } W=\\operatorname{diag}\\left(w_{1}, \\ldots, w_{n}\\right) \\\\\n",
    "\\qquad \\nabla^{2} J(\\beta)=-\\frac{1}{n} X^{\\top} W X+2 \\lambda I\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Ridge Regression (LRR)\n",
    "def solveLRR(y, X, lam):\n",
    "    n, p = X.shape\n",
    "    assert (len(y) == n)\n",
    "            \n",
    "    # Hint: Use IRLS\n",
    "    # for i in range(max_iter):\n",
    "    #     ...\n",
    "    #     beta = solveWRR(z, X, w, 2*lam)    \n",
    "    \n",
    "    # Parameters\n",
    "    max_iter = 100\n",
    "    eps = 1e-3\n",
    "    sigmoid = lambda a: 1/(1 + np.exp(-a))\n",
    "\n",
    "    # Initialize\n",
    "    beta = np.zeros(p)\n",
    "\n",
    "    # Hint: Use IRLS\n",
    "    for i in range(max_iter):\n",
    "        beta_old = beta\n",
    "        f = X.dot(beta_old)\n",
    "        w = sigmoid(f) * sigmoid(-f) # (1-sig(x)) = sig(-x)\n",
    "        z = f + y / sigmoid(y*f)\n",
    "        beta = solveWRR(z, X, w, 2*lam)\n",
    "        # Break condition (achieved convergence)\n",
    "        if np.sum((beta-beta_old)**2) < eps:\n",
    "            break\n",
    "            \n",
    "    return (beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our solver:\n",
      "[ 0.49767127  0.31536865 -0.08695284 -0.16928256  0.27290418  0.15201934\n",
      "  0.35505561  0.31972318  0.30170835 -0.39445233]\n",
      "Scikit-learn:\n",
      "[[ 0.4976808   0.31535489 -0.08695986 -0.16926035  0.27290697  0.15200484\n",
      "   0.35506729  0.31972341  0.30172333 -0.39445142]]\n",
      "\n",
      "Difference between the two:\n",
      "1.40265462156e-09\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "y_bin = np.sign(y) # Binarize targets\n",
    "lam = 0.1\n",
    "\n",
    "# Our solver\n",
    "beta1 = solveLRR(y_bin, X, lam)\n",
    "\n",
    "# Python solver\n",
    "alpha = 2 * lam * X.shape[0]\n",
    "model = lm.LogisticRegression(C=1/alpha, fit_intercept=False)\n",
    "beta2 = model.fit(X, y_bin).coef_\n",
    "\n",
    "# Check\n",
    "compare(beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Data Challenge\n",
    "\n",
    "We will try to predict whether patients have breast cancer.\n",
    "\n",
    "We use scikit-learn's [breast cancer dataset](https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset)\n",
    "\n",
    "30 features, 569 samples, 2 labels ('malignant' or 'benign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and split into training / validation sets\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model and compute its parameters\n",
    "lam = 0.01\n",
    "beta = solveLRR(y_train, X_train, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda z : 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities and classes\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "probas_pred = sigmoid(X_test.dot(beta))\n",
    "\n",
    "y_pred = np.round(probas_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model's performance:\n",
      "Accuracy: 97.87%\n",
      "AUC: 99.75%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"Our model's performance:\")\n",
    "print('Accuracy: {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('AUC: {:.2%}'.format(roc_auc_score(y_test, probas_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        67\n",
      "           1       0.99      0.98      0.98       121\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       188\n",
      "   macro avg       0.97      0.98      0.98       188\n",
      "weighted avg       0.98      0.98      0.98       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
